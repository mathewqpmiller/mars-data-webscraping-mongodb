{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission To Mars Data WebScraping Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Webscraping https://mars.nasa.gov/news/ for Article Titles and Paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Project Dependencies\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origninal Splinter Activation\n",
    "\n",
    "# from splinter import Browser\n",
    "# from selenium import webdriver\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "# browser = Browser('chrome', options=options, executable_path=ChromeDriverManager().install(), headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Selenium + BeautifulSoup\n",
    "\n",
    "Splinter’s internal use of Selenium is not compatible with the newer Selenium versions, specifically how executable_path is now handled in selenium>=4.6. Since Splinter is falling behind modern compatibility, switching to Selenium + BeautifulSoup directly is the most stable and supported approach today—especially for Jupyter Notebook projects.\n",
    "\n",
    "Here’s how to set up your scraper with Selenium + BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Updated Dependencies for Selenium + BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Set up driver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit site\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the page load\n",
    "time.sleep(5)\n",
    "\n",
    "# Parse page with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: None\n",
      "Paragraph: None\n"
     ]
    }
   ],
   "source": [
    "# Find title and paragraph\n",
    "try:\n",
    "    news_title = soup.find('div', class_='content_title').get_text(strip=True)\n",
    "    news_p = soup.find('div', class_='article_teaser_body').get_text(strip=True)\n",
    "except AttributeError:\n",
    "    news_title = None\n",
    "    news_p = None\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Output result\n",
    "print(f\"Title: {news_title}\")\n",
    "print(f\"Paragraph: {news_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapted Scraping Logic for the Article Section\n",
    "\n",
    "While scraping the latest Mars news article, I identified that the HTML structure had changed slightly from what the project originally expected.\n",
    "Rather than targeting outdated classes, I adapted the scraping logic by correctly selecting the updated title and paragraph placeholders based on the live HTML.\n",
    "This adjustment ensured accurate and resilient data extraction, reflecting real-world flexibility when dealing with evolving web page structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page loaded successfully.\n",
      "Found 10 articles.\n",
      "Title: NASA Orbiter Spots Curiosity Rover Making Tracks to Next Science Stop\n",
      "Paragraph: The image marks what may be the first time one of the agency’s Mars orbiters has captured the rover driving. NASA’s Curiosity Mars rover has never been camera shy, having been seen in selfies and images taken from space. But…\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Set up Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Visit the site\n",
    "url = \"https://mars.nasa.gov/news/\"\n",
    "driver.get(url)\n",
    "print(\"Page loaded successfully.\")\n",
    "\n",
    "try:\n",
    "    # Wait until articles are present\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"hds-content-item-inner\"))\n",
    "    )\n",
    "\n",
    "    # Get all articles\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"hds-content-item-inner\")\n",
    "    print(f\"Found {len(articles)} articles.\")\n",
    "\n",
    "    if articles:\n",
    "        first_article = articles[0]\n",
    "        # Extract title and paragraph\n",
    "        news_title = first_article.find_element(By.CLASS_NAME, \"hds-a11y-heading-22\").text\n",
    "        news_p = first_article.find_element(By.CLASS_NAME, \"margin-top-0\").text\n",
    "        print(f\"Title: {news_title}\")\n",
    "        print(f\"Paragraph: {news_p}\")\n",
    "    else:\n",
    "        print(\"No articles found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/image/featured/mars2.jpg\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import time\n",
    "\n",
    "# Set up Splinter\n",
    "browser = Browser('chrome')\n",
    "\n",
    "# Visit the URL\n",
    "url = 'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/index.html'\n",
    "browser.visit(url)\n",
    "\n",
    "# Optional: wait for page to load\n",
    "time.sleep(1)\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "html = browser.html\n",
    "page_soup = soup(html, 'html.parser')\n",
    "\n",
    "# Find the relative image URL\n",
    "relative_image_url = page_soup.find('img', class_='headerimage fade-in')['src']\n",
    "\n",
    "# Build the full URL\n",
    "featured_image_url = f'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/{relative_image_url}'\n",
    "\n",
    "# Print to confirm\n",
    "print(featured_image_url)\n",
    "\n",
    "# Close the browser\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Featured Image Retrieval\n",
    "\n",
    "In the original instructions, the task was to scrape the featured Mars image from the JPL website.\n",
    "Instead of using a complicated series of clicks or multiple browser actions, I directly accessed the full-size .jpg image by parsing the img tag with the headerimage fade-in class.\n",
    "To ensure professional quality, I also dynamically constructed a complete URL string, combining the site's base URL with the relative image path.\n",
    "This approach reduces code complexity, improves reliability, and ensures the image link remains valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featured Image URL: https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/image/featured/mars1.jpg\n"
     ]
    }
   ],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "# Initialize the browser\n",
    "browser = Browser('chrome')\n",
    "\n",
    "# Visit the JPL Featured Space Image site\n",
    "url = 'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/index.html'\n",
    "browser.visit(url)\n",
    "\n",
    "# Parse the page\n",
    "html = browser.html\n",
    "page_soup = soup(html, 'html.parser')\n",
    "\n",
    "# Find the relative path to the full-size image\n",
    "relative_image_path = page_soup.find('img', class_='headerimage fade-in')['src']\n",
    "\n",
    "# Build the full URL to the .jpg image\n",
    "base_url = url.rsplit('/', 1)[0]\n",
    "featured_image_url = f'{base_url}/{relative_image_path}'\n",
    "\n",
    "# Display the full-size image URL\n",
    "print(f'Featured Image URL: {featured_image_url}')\n",
    "\n",
    "# Close the browser\n",
    "browser.quit()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNBvb2pEHOJjtqDRbFLSvKQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
